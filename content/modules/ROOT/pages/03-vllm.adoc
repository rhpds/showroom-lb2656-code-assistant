= Building a Code Assistant with Granite and RHEL AI

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== All about vLLM

vLLM is the primary inference engine supported in RHEL AI, and a model runtime option also available in OpenShift AI. Here's a quick look at this technology:

vLLM is an open-source library designed to optimize the inference of LLMs. It uses a novel memory management approach, PagedAttention, to reduce memory usage and increase throughput. By dynamically batching incoming requests, vLLM ensures efficient hardware utilization and lower latency.

**Benefits of vLLM:**

* High Efficiency: Achieves significantly higher throughput compared to standard LLM serving methods.
* Resource-Friendly: Reduces memory requirements, making it ideal for constrained environments.
* Easy Integration: Compatible with OpenAI's API, simplifying adoption into existing workflows.
* Supported Models: vLLM supports a wide range of generative and embedding models, including Aquila, BaiChuan, ChatGLM, and Bloom-based architectures. This flexibility makes it suitable for diverse AI applications.

[#chat]
=== Chat with the Model

Now you will utilize your second terminal window to chat with the deployed model.

Once the model server is up and running, enter the following commands in the **unused** terminal window in order to chat with the Granite model you just downloaded. 

First, ensure you are running as root in this terminal window:

[source,console,role=execute,subs=attributes+]
----
sudo su -
----

Now enter the `ilab model chat` command:

[source,console,role=execute,subs=attributes+]
----
ilab model chat --model /root/.cache/instructlab/models/granite-3.1-8b-lab-v1
----

You will know you are successful when the following appears on the screen:

[source,console]
----
╭─────────────────────────────────── system ──────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-3.1-8B-LAB-V1 (type /h for help)     │
╰─────────────────────────────────────────────────────────────────────────────╯
>>>                                                                 [S][default]
----

At the chat prompt (`>>>`), feel free to chat with the model a bit. See what it knows! 

[#code_asst]
== Integrating the Granite Model into a Code Assistant

So, we have our model deployed and we've chatted with it a bit in RHEL AI - awesome! Now let's get our code assistant setup so we can do some development work. 

[#api]
=== Setup vLLM API Key

Before we go to our Visual Studio Code environment, we'll need an API key which we will use to access our deployed model. Exit out of the chat if you still have it running by typing `exit`:

[source,console,role=execute,subs=attributes+]
----
exit
----

. Create an API key that is held in $VLLM_API_KEY parameter by running the following command in a terminal window:
+
[source,console,role=execute,subs=attributes+]
----
export VLLM_API_KEY=$(python -c 'import secrets; print(secrets.token_urlsafe())')
----

. View your API key
+

[source,console,role=execute,subs=attributes+]
----
echo $VLLM_API_KEY
----

Copy this API key to your clipboard or separate document to save for the next step.

. Edit the config.yaml 
+

[source,console,role=execute,subs=attributes+]
----
ilab config edit
----

NOTE: This is a standard text file editor experience. Type `i` to edit. Once you are done with your edits, type kbd:[esc] and then `wq` to save and exit.

. Add the following params (`api-key` and `api-key-string`) to the `vllm_args` sub-section, nested within the `serve` section of the `config.yaml` file with **your api-key**. Look closely at the file to ensure correct placement. Do not remove the other values.
+

[source,console]
----
serve:
    vllm:
        vllm_args:
        - --api-key
        - <api-key-string>
----

. In the same `serve` section, change the default host_port from `127.0.0.1:8000` to `0.0.0.0:8000`:
+

[source,console]
----
serve:
    host_port: 0.0.0.0:8000
----

. Exit from the config.yaml file editor by typing kbd:[esc] and then `wq` to save and exit.

. Serve or re-serve the Granite model in one of the terminals.
+
[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /root/.cache/instructlab/models/granite-3.1-8b-lab-v1 --gpus 4
----
+

. Verify the RHEL AI server is using API key authentication by running the following command in the other terminal:
+
NOTE: The model must be running in one of the terminals before executing the following commands.
+
[source,console,role=execute,subs=attributes+]
----
ilab model chat -m ~/.cache/instructlab/models/granite-3.1-8b-lab-v1 --endpoint-url http://0.0.0.0:8000/v1
----
+

. You should see the following error:
+
[source,console]
----
Executing chat failed with: Is the server running? Error code: 401 - {'error': 'Unauthorized'}
----
+
You can also note the auth error in the output of the terminal window serving the model.

. Verify the API key works properly
+

Now, let's ensure it's working properly with the following command, using our API key:
+

[source,console,role=execute,subs=attributes+]
----
ilab model chat -m ~/.cache/instructlab/models/granite-3.1-8b-lab-v1 --endpoint-url http://0.0.0.0:8000/v1 --api-key $VLLM_API_KEY
----

Amazing! We have proven authorized access. Now we're ready to get our code extension up and running!
