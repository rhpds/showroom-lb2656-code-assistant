= Intelligent coding: Building a code assistant with Red Hat Enterprise Linux AI

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== Setting Up Visual Studio Code

Navigate back to the first **Codeserver** tab to begin working in the Visual Studio Code application.

[#install_asst]
=== Install Code Assistant Extension

Select the bottom navigation item on the left-hand side to open up the extensions marketplace.

image::extensions_tab.png[width=100%]

In the search bar, search for **Continue**. Continue is an open-source AI code assistant that integrates into various IDEs and provides autocomplete, code gen, and natural language explanations using local or remote large language models. There are quite a few code assistant options in the market today. Continue is a great, customizable option that we'll be playing around with today!

You'll see Continue as the top option, as shown below:

image::continue.png[width=100%]

Click **Install**

Once installed, click on the **settings** button as seen below. We are going to select a specific version to install since things move so quickly around here!

Click **Install Specific Version...**

image::install_specific_version.png[width=100%]

You will see a drop-down appear with different versions. Please select **v1.0.6**.

Once done installing the proper version, we're ready to configure our model connection!

[#setup_asst]
=== Setup our Code Assistant

Navigate to the Continue extension in the left-hand side navigation bar:

image::continue_tab.png[width=100%]

We will do two things in this activity:

1. Connect to very small models locally downloaded to our RHEL workstation
2. Connect our RHEL AI larger model 

In order to have all of these models available to us, we need to edit the config.yaml for Continue.

Click on the **Models** icon of the Continue extension:

image:models_button.png[width=100%]

Select **Add Models** within the menu that pops up. What you see preset here are the default models. We will adjust these in a moment.

Now, in the `config.json` file, delete all file content and copy paste the entirety of the following file into the config.json:

[source,yaml,role=execute,subs=attributes+]
----
name: Local Assistant
version: 1.0.0
schema: v1
models:
# Configure models hosted in RHEL AI (to be tested one at a time depending on which is being served by vLLM)
  - name: RHEL AI Granite 3.0 8B
    provider: vllm
    model: /var/home/ec2-user/.cache/instructlab/models/granite-3.1-8b-lab-v1
    maxPromptTokens: 2024
    apiBase: http://rhelai:8000/v1
    apiKey: <YOUR_API_KEY>
    roles:
      - chat
      - edit
      - apply
  - name: RHEL AI Qwen 2.5 7B Instruct
    provider: vllm
    model: /var/home/ec2-user/.cache/instructlab/models/Qwen/Qwen2.5-7B-Instruct
    maxPromptTokens: 2024
    apiBase: http://rhelai:8000/v1
    apiKey: <YOUR_API_KEY>
    roles:
      - chat
      - edit
      - apply
# Default model for autocompletion
  - name: Qwen2.5-Coder 1.5B
    provider: ollama
    model: qwen2.5-coder:1.5b-base
    roles:
      - autocomplete
# Default model for embedding 
  - name: Nomic Embed
    provider: ollama
    model: nomic-embed-text:latest
    roles:
      - embed
# Detect models downloaded to our RHEL workstation
  - name: Autodetect
    provider: ollama
    model: AUTODETECT
context:
  - provider: code
  - provider: docs
  - provider: diff
  - provider: terminal
  - provider: problems
  - provider: folder
  - provider: codebase
----

NOTE: Input your API key for the apiKey fields. If you did not keep your API key, you will need to retrieve it from the config.yaml (return to the RHEL AI terminal and type `ilab config show` to view the config.yaml file).

Review the configuration file and its comments to understand what this file is doing.

Continue will automatically reset. Select the model dropdown as seen below to view our available models. 

image::modellist.png[width=100%]

Feel free to chat with any of the models a bit to see how they perform. The only model that will not work is the Qwen model from the RHEL AI machine (you'll have to go back and serve that one instead of Granite for it to be activated here).

Note the performance differences betweem the local smaller models and our RHEL AI Granite model.

Now, let's get to work with some real coding exercises.

